{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBR geometry analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "import os\n",
    "import open3d as o3d\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4648369, 4)\n",
      "Number of points: 4648369\n",
      "Number of semantics: 6658\n"
     ]
    }
   ],
   "source": [
    "def read_las_file(file_path):\n",
    "    \n",
    "    # read las file\n",
    "    las_file = laspy.read(file_path)\n",
    "\n",
    "    # get the point data\n",
    "    point_data = las_file.points\n",
    "    # get the x, y, z coordinates\n",
    "    x = point_data.x\n",
    "    y = point_data.y\n",
    "    z = point_data.z\n",
    "    # get the intensity values\n",
    "    semantics = point_data.intensity\n",
    "\n",
    "    # stock the x, y, z, semantics in a numpy array\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    semantics = np.array(semantics)\n",
    "\n",
    "    # get color values from las file\n",
    "    red = point_data.red\n",
    "    green = point_data.green\n",
    "    blue = point_data.blue\n",
    "\n",
    "    colors = np.array([red, green, blue]).T\n",
    "\n",
    "    # stack the arrays\n",
    "    points_semantics_source = np.vstack((x, y, z, semantics)).T\n",
    "\n",
    "    print(points_semantics_source.shape)\n",
    "\n",
    "    # assert the length of the arrays\n",
    "    assert len(x) == len(y) == len(z) == len(semantics), \"Length of x, y, z, and intensity arrays do not match.\"\n",
    "\n",
    "    # print the numbers of points and semantics\n",
    "    print(f\"Number of points: {len(x)}\")\n",
    "    print(f\"Number of semantics: {np.unique(semantics).size}\")\n",
    "\n",
    "    return points_semantics_source, colors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_points_to_las(points, color, filename):\n",
    "    # Create a new LAS header and file\n",
    "    header = laspy.LasHeader(point_format=3, version=\"1.2\")\n",
    "    las_file = laspy.LasData(header)\n",
    "\n",
    "    # Set coordinates\n",
    "    las_file.x = points[:, 0]\n",
    "    las_file.y = points[:, 1]\n",
    "    las_file.z = points[:, 2]\n",
    "\n",
    "    # Handle intensity (semantics)\n",
    "    semantics = points[:, 3].astype(np.int32)  # Promote to signed int\n",
    "    max_intensity = semantics[semantics != -1].max()\n",
    "    semantics[semantics == -1] = max_intensity + 1  # Set outliers to new value\n",
    "    las_file.intensity = semantics.astype(np.uint16)  # Cast back to uint16\n",
    "\n",
    "    print(f\"Max intensity: {max_intensity}\")\n",
    "\n",
    "    # Set RGB color\n",
    "    las_file.red = color[:, 0].astype(np.uint16)\n",
    "    las_file.green = color[:, 1].astype(np.uint16)\n",
    "    las_file.blue = color[:, 2].astype(np.uint16)\n",
    "\n",
    "    # Write the LAS file\n",
    "    las_file.write(filename)\n",
    "\n",
    "\n",
    "\n",
    "def sor_filter_parallel(points_semantics, nb_neighbors=10, std_ratio=0.6, n_jobs=8):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "\n",
    "    def process_semantic_group(points_semantics, semantics_id, nb_neighbors, std_ratio):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "\n",
    "        # Create Open3D point cloud\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "\n",
    "        # Apply SOR filter\n",
    "        _, inliers = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors, std_ratio=std_ratio)\n",
    "        inliers = np.asarray(inliers)\n",
    "\n",
    "        # Identify outliers (not in inliers)\n",
    "        all_indices = np.arange(len(indices))\n",
    "        outlier_indices = np.setdiff1d(all_indices, inliers)\n",
    "\n",
    "        # Return global outlier indices to be set to -1\n",
    "        return indices[outlier_indices]\n",
    "    \n",
    "    # Run SOR in parallel for each semantic group\n",
    "    outlier_indices_all = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_semantic_group)(points_semantics, sid, nb_neighbors, std_ratio)\n",
    "        for sid in semantics_ids\n",
    "    )\n",
    "\n",
    "    # Flatten and mark all outliers with semantic = -1\n",
    "    all_outliers = np.concatenate(outlier_indices_all)\n",
    "    points_semantics[all_outliers, 3] = -1\n",
    "\n",
    "    # Print the number of outliers\n",
    "    print(f\"Number of outliers from SOR filter: {len(all_outliers)}\")\n",
    "\n",
    "    return points_semantics\n",
    "\n",
    "\n",
    "def cluster_filter(points_semantics, eps=0.5, min_samples=10, n_jobs=8):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "    def process_semantic_group(sem_id):\n",
    "        # Find global indices for this semantic group\n",
    "        group_indices = np.where(points_semantics[:, 3] == sem_id)[0]\n",
    "        group_points = points_semantics[group_indices, :3]\n",
    "\n",
    "        if len(group_points) < min_samples:\n",
    "            return group_indices  # All treated as outliers\n",
    "\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(group_points)\n",
    "        labels, counts = np.unique(clustering.labels_, return_counts=True)\n",
    "\n",
    "        # Ignore noise-only cases (no valid clusters)\n",
    "        if np.all(labels == -1):\n",
    "            return group_indices\n",
    "\n",
    "        largest_cluster_label = labels[np.argmax(counts)]\n",
    "        inliers_local = np.where(clustering.labels_ == largest_cluster_label)[0]\n",
    "        all_local = np.arange(len(group_indices))\n",
    "        outliers_local = np.setdiff1d(all_local, inliers_local)\n",
    "\n",
    "        # Return global indices of outliers\n",
    "        return group_indices[outliers_local]\n",
    "\n",
    "    # Parallel loop over semantic IDs\n",
    "    outlier_indices_all = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_semantic_group)(sid) for sid in semantics_ids\n",
    "    )\n",
    "\n",
    "    all_outlier_indices = np.concatenate(outlier_indices_all)\n",
    "    points_semantics[all_outlier_indices, 3] = -1\n",
    "\n",
    "    # print the number of outliers\n",
    "    print(f\"Number of outliers from DBSCAN filter: {len(all_outlier_indices)}\")\n",
    "\n",
    "    return points_semantics\n",
    "\n",
    "\n",
    "\n",
    "def size_filter(points_semantics, min_horizontal_length=0.5, max_horizontal_length=5.0, min_vertical_length=0.5, max_vertical_length=5.0):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "    def process_semantic_group(points_semantics, semantics_id, min_horizontal_length, max_horizontal_length, min_vertical_length, max_vertical_length):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "\n",
    "        # Calculate the bounding box\n",
    "        min_x, min_y, min_z = np.min(xyz, axis=0)\n",
    "        max_x, max_y, max_z = np.max(xyz, axis=0)\n",
    "\n",
    "        # Calculate lengths\n",
    "        horizontal_length = np.sqrt((max_x - min_x) ** 2 + (max_y - min_y) ** 2)\n",
    "        vertical_length = max_z - min_z\n",
    "\n",
    "        # return True if the lengths are within the specified range; otherwise, return False\n",
    "        if min_horizontal_length <= horizontal_length <= max_horizontal_length and min_vertical_length <= vertical_length <= max_vertical_length:\n",
    "            return semantics_id, True\n",
    "        else:\n",
    "            return semantics_id, False\n",
    "        \n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(process_semantic_group)(points_semantics, sid, min_horizontal_length, max_horizontal_length, min_vertical_length, max_vertical_length)\n",
    "        for sid in semantics_ids\n",
    "    )\n",
    "\n",
    "    # set the semantics to -1 for the points that do not pass the filter\n",
    "    for semantics_id, keep in results:\n",
    "        if not keep:\n",
    "            indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "            points_semantics[indices, 3] = -1\n",
    "\n",
    "    # print the number of semantics_ids that passed the filter\n",
    "    print(f\"Number of semantics_ids that passed the size filter: {len(np.unique(points_semantics[:, 3]))}\")\n",
    "    # print the number of semantics_ids that did not pass the filter\n",
    "    print(f\"Number of semantics_ids that did not pass the size filter: {len(semantics_ids) - len(np.unique(points_semantics[:, 3]))}\")\n",
    "    return points_semantics\n",
    "\n",
    "\n",
    "def pca_curvature_filter(points_semantics, curvature_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Filter points based on PCA planarity.\n",
    "    :param points_semantics: numpy array of shape (N, 4) where N is the number of points\n",
    "    :param planarity_threshold: threshold for planarity; larger values indicate flatter surfaces; which means only very flat surfaces are removed\n",
    "    \"\"\"\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "    def process_semantic_group(points_semantics, semantics_id, ratio):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "\n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(xyz)\n",
    "\n",
    "        eigenvalues = pca.explained_variance_  # λ₁, λ₂, λ₃\n",
    "\n",
    "        # Sort eigenvalues from largest to smallest\n",
    "        eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "        λ1, λ2, λ3 = eigenvalues\n",
    "\n",
    "\n",
    "        # Avoid divide by zero\n",
    "        if λ1 == 0:\n",
    "            return 0.0  # when all points are the same\n",
    "\n",
    "        curvature = λ3 / (λ1 + λ2 + λ3)  \n",
    "\n",
    "        if semantics_id == 5166:\n",
    "            print(f\"semantics_id: {semantics_id}, λ1: {λ1}, λ2: {λ2}, λ3: {λ3}, curvature: {curvature}\")\n",
    "        \n",
    "        if curvature > ratio:   # less flat\n",
    "            return semantics_id, True\n",
    "        else:  # more flat\n",
    "            return semantics_id, False\n",
    "        \n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(process_semantic_group)(points_semantics, sid, curvature_threshold)\n",
    "        for sid in semantics_ids\n",
    "    )\n",
    "    # set the semantics to -1 for the points that do not pass the filter\n",
    "    for semantics_id, keep in results:\n",
    "        if not keep:\n",
    "            indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "            points_semantics[indices, 3] = -1\n",
    "    # print the number of semantics_ids that passed the filter\n",
    "    print(f\"Number of semantics_ids that passed the PCA filter: {len(np.unique(points_semantics[:, 3]))}\")\n",
    "    # print the number of semantics_ids that did not pass the filter\n",
    "    print(f\"Number of semantics_ids that did not pass the PCA filter: {len(semantics_ids) - len(np.unique(points_semantics[:, 3]))}\")\n",
    "    return points_semantics\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4648369, 4)\n",
      "Number of points: 4648369\n",
      "Number of semantics: 5758\n",
      "semantics_id: 5166.0, λ1: 0.8960413313045098, λ2: 0.17898183871062104, λ3: 0.0001824252738268768, curvature: 0.0001696654803752678\n",
      "Number of semantics_ids that passed the PCA filter: 113\n",
      "Number of semantics_ids that did not pass the PCA filter: 5645\n",
      "Max intensity: 6518\n"
     ]
    }
   ],
   "source": [
    "# points_semantics_source, colors = read_las_file(\"data/centennial_bluff_mission_a_0.las\")\n",
    "# points_semantics = points_semantics_source.copy()\n",
    "# #points_semantics = sor_filter_parallel(points_semantics, nb_neighbors=6, std_ratio=1.0, n_jobs=8)\n",
    "# points_semantics = cluster_filter(points_semantics, eps=0.5, min_samples=10, n_jobs=8)\n",
    "# points_semantics = size_filter(points_semantics, min_horizontal_length=0.3, max_horizontal_length=4.0, min_vertical_length=0.3, max_vertical_length=4.0)\n",
    "# #save_points_to_las(points_semantics, colors, \"data/centennial_bluff_mission_a_0_size_filtered.las\")\n",
    "\n",
    "points_semantics_source, colors = read_las_file(\"data/centennial_bluff_mission_a_0_size_filtered.las\")\n",
    "points_semantics = points_semantics_source.copy()\n",
    "\n",
    "points_semantics = pca_curvature_filter(points_semantics, planarity_threshold=0.15)\n",
    "save_points_to_las(points_semantics, colors, \"data/centennial_bluff_mission_a_0_filtered.las\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pbr_statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
