{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBR geometry analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import laspy\n",
    "import numpy as np\n",
    "import os\n",
    "import open3d as o3d\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_las_file(file_path):\n",
    "    \n",
    "    # read las file\n",
    "    las_file = laspy.read(file_path)\n",
    "\n",
    "    # get the point data\n",
    "    point_data = las_file.points\n",
    "    # get the x, y, z coordinates\n",
    "    x = point_data.x\n",
    "    y = point_data.y\n",
    "    z = point_data.z\n",
    "    # get the intensity values\n",
    "    semantics = point_data.intensity\n",
    "\n",
    "    # stock the x, y, z, semantics in a numpy array\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z = np.array(z)\n",
    "    semantics = np.array(semantics)\n",
    "\n",
    "    # get color values from las file\n",
    "    red = point_data.red\n",
    "    green = point_data.green\n",
    "    blue = point_data.blue\n",
    "\n",
    "    colors = np.array([red, green, blue]).T\n",
    "\n",
    "    # stack the arrays\n",
    "    points_semantics_source = np.vstack((x, y, z, semantics)).T\n",
    "\n",
    "    print(points_semantics_source.shape)\n",
    "\n",
    "    # assert the length of the arrays\n",
    "    assert len(x) == len(y) == len(z) == len(semantics), \"Length of x, y, z, and intensity arrays do not match.\"\n",
    "\n",
    "    # print the numbers of points and semantics\n",
    "    print(f\"Number of points: {len(x)}\")\n",
    "    print(f\"Number of semantics: {np.unique(semantics).size}\")\n",
    "\n",
    "    return points_semantics_source, colors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_points_to_las(points, color, filename):\n",
    "    # Create a new LAS header and file\n",
    "    header = laspy.LasHeader(point_format=3, version=\"1.2\")\n",
    "    las_file = laspy.LasData(header)\n",
    "\n",
    "    # Set coordinates\n",
    "    las_file.x = points[:, 0]\n",
    "    las_file.y = points[:, 1]\n",
    "    las_file.z = points[:, 2]\n",
    "\n",
    "    # Handle intensity (semantics)\n",
    "    semantics = points[:, 3].astype(np.int16)  # Promote to signed int\n",
    "    max_intensity = semantics[semantics != -1].max()\n",
    "\n",
    "    if len(semantics[semantics == max_intensity]) < len(semantics)/10:\n",
    "        semantics[semantics == -1] = max_intensity + 1 \n",
    "        print(f\"Max intensity: {max_intensity + 1}\")\n",
    "    else:\n",
    "        semantics[semantics == -1] = max_intensity \n",
    "        print(f\"Max intensity: {max_intensity}\")\n",
    "    las_file.intensity = semantics.astype(np.uint16)  # Cast back to uint16\n",
    "\n",
    "    \n",
    "\n",
    "    # Set RGB color\n",
    "    las_file.red = color[:, 0].astype(np.uint16)\n",
    "    las_file.green = color[:, 1].astype(np.uint16)\n",
    "    las_file.blue = color[:, 2].astype(np.uint16)\n",
    "\n",
    "    # Write the LAS file\n",
    "    las_file.write(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cluster_filter(points_semantics, eps=0.5, min_samples=10, n_jobs=8):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "    def process_semantic_group(sem_id):\n",
    "        # Find global indices for this semantic group\n",
    "        group_indices = np.where(points_semantics[:, 3] == sem_id)[0]\n",
    "        group_points = points_semantics[group_indices, :3]\n",
    "\n",
    "        if len(group_points) < min_samples:\n",
    "            return group_indices  # All treated as outliers\n",
    "\n",
    "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(group_points)\n",
    "        labels, counts = np.unique(clustering.labels_, return_counts=True)\n",
    "\n",
    "        # Ignore noise-only cases (no valid clusters)\n",
    "        if np.all(labels == -1):\n",
    "            return group_indices\n",
    "\n",
    "        largest_cluster_label = labels[np.argmax(counts)]\n",
    "        inliers_local = np.where(clustering.labels_ == largest_cluster_label)[0]\n",
    "        all_local = np.arange(len(group_indices))\n",
    "        outliers_local = np.setdiff1d(all_local, inliers_local)\n",
    "\n",
    "        # Return global indices of outliers\n",
    "        return group_indices[outliers_local]\n",
    "\n",
    "    # Parallel loop over semantic IDs\n",
    "    outlier_indices_all = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_semantic_group)(sid) for sid in semantics_ids\n",
    "    )\n",
    "\n",
    "    all_outlier_indices = np.concatenate(outlier_indices_all)\n",
    "    points_semantics[all_outlier_indices, 3] = -1\n",
    "\n",
    "    # print the number of outliers\n",
    "    print(f\"Number of outliers from DBSCAN filter: {len(all_outlier_indices)}\")\n",
    "\n",
    "    return points_semantics\n",
    "\n",
    "\n",
    "\n",
    "def size_filter(points_semantics, min_horizontal_length=0.5, max_horizontal_length=5.0, min_vertical_length=0.5, max_vertical_length=5.0):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "    def process_semantic_group(points_semantics, semantics_id, min_horizontal_length, max_horizontal_length, min_vertical_length, max_vertical_length):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "\n",
    "        # Calculate the bounding box\n",
    "        min_x, min_y, min_z = np.min(xyz, axis=0)\n",
    "        max_x, max_y, max_z = np.max(xyz, axis=0)\n",
    "\n",
    "        # Calculate lengths\n",
    "        horizontal_length = np.sqrt((max_x - min_x) ** 2 + (max_y - min_y) ** 2)\n",
    "        vertical_length = max_z - min_z\n",
    "\n",
    "        # return True if the lengths are within the specified range; otherwise, return False\n",
    "        if min_horizontal_length <= horizontal_length <= max_horizontal_length and min_vertical_length <= vertical_length <= max_vertical_length:\n",
    "            return semantics_id, True\n",
    "        else:\n",
    "            return semantics_id, False\n",
    "        \n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(process_semantic_group)(points_semantics, sid, min_horizontal_length, max_horizontal_length, min_vertical_length, max_vertical_length)\n",
    "        for sid in semantics_ids\n",
    "    )\n",
    "\n",
    "    # set the semantics to -1 for the points that do not pass the filter\n",
    "    for semantics_id, keep in results:\n",
    "        if not keep:\n",
    "            indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "            points_semantics[indices, 3] = -1\n",
    "\n",
    "    # print the number of semantics_ids that passed the filter\n",
    "    print(f\"Number of semantics_ids that passed the size filter: {len(np.unique(points_semantics[:, 3]))-1}\")\n",
    "    # print the number of semantics_ids that did not pass the filter\n",
    "    print(f\"Number of semantics_ids that did not pass the size filter: {len(semantics_ids) - len(np.unique(points_semantics[:, 3]))+1}\")\n",
    "    return points_semantics\n",
    "\n",
    "\n",
    "def pca_curvature_filter(points_semantics, curvature_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Filter points based on PCA planarity.\n",
    "    :param points_semantics: numpy array of shape (N, 4) where N is the number of points\n",
    "    :param curvature_threshold: float, threshold for curvature; large values indicate more objects will be removed\n",
    "    \"\"\"\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "\n",
    "    def process_semantic_group(points_semantics, semantics_id, ratio):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "\n",
    "        # Perform PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(xyz)\n",
    "\n",
    "        eigenvalues = pca.explained_variance_  # λ₁, λ₂, λ₃\n",
    "\n",
    "        # Sort eigenvalues from largest to smallest\n",
    "        eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "        λ1, λ2, λ3 = eigenvalues\n",
    "\n",
    "\n",
    "        # Avoid divide by zero\n",
    "        if λ1 == 0:\n",
    "            return 0.0  # when all points are the same\n",
    "\n",
    "        curvature = λ3 / (λ1 + λ2 + λ3)  \n",
    "        \n",
    "        if curvature > ratio:   # less flat\n",
    "            return semantics_id, True\n",
    "        else:  # more flat\n",
    "            return semantics_id, False\n",
    "        \n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(process_semantic_group)(points_semantics, sid, curvature_threshold)\n",
    "        for sid in semantics_ids\n",
    "    )\n",
    "    # set the semantics to -1 for the points that do not pass the filter\n",
    "    for semantics_id, keep in results:\n",
    "        if not keep:\n",
    "            indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "            points_semantics[indices, 3] = -1\n",
    "    # print the number of semantics_ids that passed the filter\n",
    "    print(f\"Number of semantics_ids that passed the PCA filter: {len(np.unique(points_semantics[:, 3]))-1}\")\n",
    "    # print the number of semantics_ids that did not pass the filter\n",
    "    print(f\"Number of semantics_ids that did not pass the PCA filter: {len(semantics_ids) - len(np.unique(points_semantics[:, 3]))+1}\")\n",
    "    return points_semantics\n",
    "\n",
    "def density_filter(points_semantics, semantics_id, radius=0.1, density_threshold=0.5):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "    \n",
    "    def compute_point_density(points_semantics, semantics_id, radius=0.1, density_threshold=0.5):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "        # Create Open3D point cloud\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "        # Compute point density\n",
    "        pcd_tree = o3d.geometry.KDTreeFlann(pcd)\n",
    "        densities = []\n",
    "        points = np.asarray(pcd.points)\n",
    "        \n",
    "        for i in range(len(points)):\n",
    "            [k, idx, _] = pcd_tree.search_radius_vector_3d(pcd.points[i], radius)\n",
    "            densities.append(k)\n",
    "\n",
    "        density = np.mean(densities)\n",
    "                \n",
    "        if density >= density_threshold:\n",
    "            return semantics_id, True\n",
    "        else:\n",
    "            return semantics_id, False\n",
    "        \n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(compute_point_density)(points_semantics, sid, radius, density_threshold)\n",
    "        for sid in semantics_ids\n",
    "    )\n",
    "    # set the semantics to -1 for the points that do not pass the filter\n",
    "    for semantics_id, keep in results:\n",
    "        if not keep:\n",
    "            indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "            points_semantics[indices, 3] = -1\n",
    "    # print the number of semantics_ids that passed the filter\n",
    "    print(f\"Number of semantics_ids that passed the density filter: {len(np.unique(points_semantics[:, 3]))-1}\")\n",
    "    # print the number of semantics_ids that did not pass the filter\n",
    "    print(f\"Number of semantics_ids that did not pass the density filter: {len(semantics_ids) - len(np.unique(points_semantics[:, 3]))+1}\")\n",
    "    return points_semantics\n",
    "\n",
    "\n",
    "def hwr_filter(points_semantics, hwr_threshold=10.0):\n",
    "    semantics_ids = np.unique(points_semantics[:, 3])\n",
    "    \n",
    "    def compute_hwr(points_semantics, semantics_id):\n",
    "        # Get indices of points with this semantics\n",
    "        indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "        local_points_semantics = points_semantics[indices]\n",
    "        xyz = local_points_semantics[:, :3]\n",
    "\n",
    "        pca = PCA(n_components=3)\n",
    "        pca.fit(xyz)\n",
    "\n",
    "        # Get the eigenvalues\n",
    "        eigenvalues = pca.explained_variance_\n",
    "        # Get the eigenvectors\n",
    "        eigenvectors = pca.components_\n",
    "\n",
    "        # find the eigenvector aligned with the z-axis\n",
    "        z_axis = np.array([0, 0, 1])\n",
    "        # find the angle between the eigenvector and the z-axis\n",
    "        angles = np.arccos(np.clip(np.dot(eigenvectors, z_axis), -1.0, 1.0))\n",
    "        # find the index of the eigenvector aligned with the z-axis\n",
    "        index = np.argmax(angles)\n",
    "        # find the eigenvalue associated with the eigenvector aligned with the z-axis\n",
    "        height = eigenvalues[index]\n",
    "        # for the remaining eigenvalues, find the smaller one\n",
    "        eigenvalues = np.delete(eigenvalues, index)\n",
    "        width = np.min(eigenvalues)\n",
    "        # compute the height to width ratio\n",
    "        hwr = height / width\n",
    "        return semantics_id, hwr\n",
    "    \n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=8)(\n",
    "        delayed(compute_hwr)(points_semantics, sid)\n",
    "        for sid in semantics_ids if sid != -1\n",
    "    )\n",
    "\n",
    "    # set the semantics to -1 for the points that do not pass the filter\n",
    "    for semantics_id, hwr in results:\n",
    "        if hwr > hwr_threshold:\n",
    "            print(semantics_id, hwr)\n",
    "            indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "            points_semantics[indices, 3] = -1\n",
    "\n",
    "    hwr_results = [hwr for _, hwr in results if hwr > hwr_threshold]\n",
    "    print(hwr_results)\n",
    "\n",
    "    # print the number of semantics_ids that passed the filter\n",
    "    print(f\"Number of semantics_ids that passed the HWR filter: {len(np.unique(points_semantics[:, 3]))-1}\")\n",
    "    # print the number of semantics_ids that did not pass the filter\n",
    "    print(f\"Number of semantics_ids that did not pass the HWR filter: {len(semantics_ids) - len(np.unique(points_semantics[:, 3]))+1}\")\n",
    "    return points_semantics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4648369, 4)\n",
      "Number of points: 4648369\n",
      "Number of semantics: 6658\n",
      "Number of outliers from DBSCAN filter: 264077\n",
      "Number of semantics_ids that passed the size filter: 5845\n",
      "Number of semantics_ids that did not pass the size filter: 814\n",
      "Max intensity: 6568\n",
      "(4648369, 4)\n",
      "Number of points: 4648369\n",
      "Number of semantics: 5846\n",
      "Number of semantics_ids that passed the PCA filter: 93\n",
      "Number of semantics_ids that did not pass the PCA filter: 5753\n",
      "Max intensity: 6518\n",
      "(4648369, 4)\n",
      "Number of points: 4648369\n",
      "Number of semantics: 94\n",
      "Number of semantics_ids that passed the density filter: 44\n",
      "Number of semantics_ids that did not pass the density filter: 50\n",
      "Max intensity: 6519\n",
      "(4648369, 4)\n",
      "Number of points: 4648369\n",
      "Number of semantics: 44\n",
      "[]\n",
      "Number of semantics_ids that passed the HWR filter: 43\n",
      "Number of semantics_ids that did not pass the HWR filter: 1\n",
      "Max intensity: 6519\n"
     ]
    }
   ],
   "source": [
    "points_semantics_source, colors = read_las_file(\"data/centennial_bluff_mission_a_0.las\")\n",
    "points_semantics = points_semantics_source.copy()\n",
    "points_semantics = cluster_filter(points_semantics, eps=0.3, min_samples=10, n_jobs=8)\n",
    "points_semantics = size_filter(points_semantics, min_horizontal_length=0.3, max_horizontal_length=4.0, min_vertical_length=0.3, max_vertical_length=4.0)\n",
    "save_points_to_las(points_semantics, colors, \"data/centennial_bluff_mission_a_0_size_filtered.las\")\n",
    "\n",
    "points_semantics_source, colors = read_las_file(\"data/centennial_bluff_mission_a_0_size_filtered.las\")\n",
    "points_semantics = points_semantics_source.copy()\n",
    "points_semantics = pca_curvature_filter(points_semantics, curvature_threshold=0.15)\n",
    "save_points_to_las(points_semantics, colors, \"data/centennial_bluff_mission_a_0_pca_filtered.las\")\n",
    "\n",
    "\n",
    "points_semantics_source, colors = read_las_file(\"data/centennial_bluff_mission_a_0_pca_filtered.las\")\n",
    "points_semantics = points_semantics_source.copy()\n",
    "points_semantics = density_filter(points_semantics, semantics_id=1, radius=0.1, density_threshold=4.6)\n",
    "save_points_to_las(points_semantics, colors, \"data/centennial_bluff_mission_a_0_density_filtered.las\")\n",
    "\n",
    "points_semantics_source, colors = read_las_file(\"data/centennial_bluff_mission_a_0_density_filtered.las\")\n",
    "points_semantics = points_semantics_source.copy()\n",
    "points_semantics = hwr_filter(points_semantics, hwr_threshold=4.0)\n",
    "save_points_to_las(points_semantics, colors, \"data/centennial_bluff_mission_a_0_hwr_filtered.las\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_point_density(points_semantics, semantics_id, radius=0.1, density_threshold=0.5):\n",
    "    # Get indices of points with this semantics\n",
    "    indices = np.where(points_semantics[:, 3] == semantics_id)[0]\n",
    "    local_points_semantics = points_semantics[indices]\n",
    "    xyz = local_points_semantics[:, :3]\n",
    "    # Create Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "    # Compute point density\n",
    "    pcd_tree = o3d.geometry.KDTreeFlann(pcd)\n",
    "    densities = []\n",
    "    points = np.asarray(pcd.points)\n",
    "    \n",
    "    for i in range(len(points)):\n",
    "        [k, idx, _] = pcd_tree.search_radius_vector_3d(pcd.points[i], radius)\n",
    "        densities.append(k)\n",
    "\n",
    "    density = np.mean(densities)\n",
    "\n",
    "    print(f\"Density: {density}\")\n",
    "            \n",
    "    if density >= density_threshold:\n",
    "        return semantics_id, True\n",
    "    else:\n",
    "        return semantics_id, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density: 5.005702066999287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5932, True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_point_density(points_semantics, semantics_id=5932, radius=0.1, density_threshold=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
